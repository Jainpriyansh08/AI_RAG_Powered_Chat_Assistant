# AI RAG Powered Chat Assistant

This repository implements a production-grade **Retrieval-Augmented Generation (RAG)** system designed for querying unstructured datasets (PDFs) with high accuracy.

It demonstrates advanced techniques including **Hybrid Search**, **Metadata Filtering**, and **Context-Aware Retrieval**.

## üèó Architecture

- **Orchestration**: LangChain
- **Vector Database**: FAISS (Dense semantic search)
- **Keyword Search**: BM25 (Sparse lexical search)
- **Model**: OpenAI GPT-3.5-turbo
- **Interface**: Streamlit

## üöÄ Key Features

### 1. Hybrid Search (Ensemble Retrieval)

Instead of relying solely on Vector Similarity (which can miss specific keywords), this project uses an **Ensemble Retriever**.

- Combines **Dense Vector Search** (FAISS) with **Sparse Keyword Search** (BM25).
- Uses weighted ranking (Reciprocal Rank Fusion concept) to prioritize the best results from both methods.

### 2. Intelligent Chunking Strategy

- **Chunk Size**: 1000 tokens (approx 750 words).
- **Overlap**: 200 tokens.
- Ensures context is preserved across split boundaries to prevent information loss during retrieval.

### 3. Source Citations

- The system employs a "Trust but Verify" approach. Every answer generated by the LLM is accompanied by the specific source document chunks used to generate it.

## üõ† Installation & Usage

1. **Clone the repo**

   ```bash
   git clone https://github.com/Jainpriyansh08/AI_RAG_Powered_Chat_Assistant.git
   cd ai_rag_powered_chat_assistant
   ```

2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Set up API Keys**  
   Create a `.env` file and add your OpenAI Key:

   ```bash
   OPENAI_API_KEY=sk-your-key-here
   ```

4. **Ingest Data**  
   Place your PDF files in the `data/` folder and run the ingestion pipeline:

   ```bash
   python ingest.py
   ```

5. **Run the App**

   ```bash
   streamlit run app.py
   ```
